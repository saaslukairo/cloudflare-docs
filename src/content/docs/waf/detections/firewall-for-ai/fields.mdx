---
pcx_content_type: reference
title: Firewall for AI fields
tags:
  - AI
sidebar:
  order: 4
  label: Available fields
---

import { Type } from "~/components";

When enabled, Firewall for AI populates the following fields:

| Field                                                                                                             | Description                                                                                                                                                                                                                              |
| ----------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| LLM PII detected <br/> [`cf.llm.prompt.pii_detected`][1] <br/> <Type text="Boolean"/>                             | Indicates whether any personally identifiable information (PII) has been detected in the LLM prompt included in the request.                                                                                                             |
| LLM PII categories <br/> [`cf.llm.prompt.pii_categories`][2] <br/> <Type text="Array<String>"/>                   | Array of string values with the personally identifiable information (PII) categories found in the LLM prompt included in the request.<br/>[Category list](/ruleset-engine/rules-language/fields/reference/cf.llm.prompt.pii_categories/) |
| LLM Content detected <br/> [`cf.llm.prompt.detected`][3] <br/> <Type text="Boolean "/>                            | Indicates whether Cloudflare detected an LLM prompt in the incoming request.                                                                                                                                                             |
| LLM Unsafe topic detected <br/> [`cf.llm.prompt.unsafe_topic_detected`][4] <br/> <Type text="Boolean"/>           | Indicates whether the incoming request includes any unsafe topic category in the LLM prompt.                                                                                                                                             |
| LLM Unsafe topic categories <br/> [`cf.llm.prompt.unsafe_topic_categories`][5] <br/> <Type text="Array<String>"/> | Array of string values with the type of unsafe topics detected in the LLM prompt.<br/>[Category list](/ruleset-engine/rules-language/fields/reference/cf.llm.prompt.unsafe_topic_categories/)                                            |
| LLM Injection score <br/> [`cf.llm.prompt.injection_score`][6] <br/> <Type text="Number"/>                        | A score from 1â€“99 that represents the likelihood that the LLM prompt in the request is trying to perform a prompt injection attack.                                                                                                      |

[1]: /ruleset-engine/rules-language/fields/reference/cf.llm.prompt.pii_detected/
[2]: /ruleset-engine/rules-language/fields/reference/cf.llm.prompt.pii_categories/
[3]: /ruleset-engine/rules-language/fields/reference/cf.llm.prompt.detected/
[4]: /ruleset-engine/rules-language/fields/reference/cf.llm.prompt.unsafe_topic_detected/
[5]: /ruleset-engine/rules-language/fields/reference/cf.llm.prompt.unsafe_topic_categories/
[6]: /ruleset-engine/rules-language/fields/reference/cf.llm.prompt.injection_score/
