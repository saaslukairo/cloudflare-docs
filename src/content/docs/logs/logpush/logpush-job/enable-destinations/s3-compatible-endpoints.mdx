---
title: Enable S3-compatible endpoints
pcx_content_type: how-to
sidebar:
  order: 57
head:
  - tag: title
    content: Enable Logpush to S3-compatible endpoints
---

import { Render, APIRequest } from "~/components";

Cloudflare Logpush supports pushing logs to S3-compatible destinations via the Cloudflare dashboard or via API, including:

- [Alibaba Cloud OSS](https://www.alibabacloud.com/help/doc-detail/64919.htm#title-37m-7gl-xy2)
- [Backblaze B2](https://www.backblaze.com/b2/docs/s3_compatible_api.html)
- [DigitalOcean Spaces](https://www.digitalocean.com/docs/spaces/)
- [IBM Cloud Object Storage](https://cloud.ibm.com/apidocs/cos/cos-compatibility)
- [JD Cloud Object Storage Service](https://docs.jdcloud.com/en/object-storage-service/introduction-2)
- [Linode Object Storage](https://www.linode.com/products/object-storage/)
- [Oracle Cloud Object Storage](https://docs.cloud.oracle.com/en-us/iaas/Content/Object/Tasks/s3compatibleapi.htm)
- On-premise [Ceph Object Gateway](https://docs.ceph.com/en/latest/radosgw/s3/)

For more information about Logpush and the current production APIs, refer to [Cloudflare Logpush](/logs/logpush/) documentation.

## Manage via the Cloudflare dashboard

<Render file="enable-logpush-job" product="logs" />

4. In **Select a destination**, choose **S3-Compatible**.

5. Enter or select the following destination information:
   - **Bucket** - S3 Compatible bucket name
   - **Path** - bucket location within the storage container
   - **Organize logs into daily subfolders** (recommended)
   - **Endpoint URL** - The URL without the bucket name or path. Example, `sfo2.digitaloceanspaces.com`.
   - **Bucket region**
   - **Access Key ID**
   - **Secret Access Key**

When you are done entering the destination details, select **Continue**.

6. Select the dataset to push to the storage service.

7. In the next step, you need to configure your logpush job:
   - Enter the **Job name**.
   - Under **If logs match**, you can select the events to include and/or remove from your logs. Refer to [Filters](/logs/logpush/logpush-job/filters/) for more information. Not all datasets have this option available.
   - In **Send the following fields**, you can choose to either push all logs to your storage destination or selectively choose which logs you want to push.

8. In **Advanced Options**, you can:
   - Choose the format of timestamp fields in your logs (`RFC3339`(default),`Unix`, or `UnixNano`).
   - Select a [sampling rate](/logs/logpush/logpush-job/api-configuration/#sampling-rate) for your logs or push a randomly-sampled percentage of logs.
   - Enable redaction for `CVE-2021-44228`. This option will replace every occurrence of `${` with `x{`.

9. Select **Submit** once you are done configuring your logpush job.

## Manage via API

To set up S3-compatible endpoints:

1. Create a job with the appropriate endpoint URL and authentication parameters.
2. Enable the job to begin pushing logs.

:::note[Note]

Unlike Logpush jobs to Amazon S3, there is no ownership challenge with S3-compatible APIs.

:::

<Render file="enable-read-permissions" product="logs" />

### 1. Create a job

To create a job, make a `POST` request to the Logpush jobs endpoint with the following fields:

- **name** (optional) - Use your domain name as the job name.
- **destination_conf** - A log destination consisting of an endpoint name, bucket name, bucket path, region, access-key-id, and secret-access-key in the following string format:

```bash
"s3://<BUCKET_NAME>/<BUCKET_PATH>?region=<REGION>&access-key-id=<ACCESS_KEY_ID>&secret-access-key=<SECRET_ACCESS_KEY>&endpoint=<ENDPOINT_URL>"
```

:::note[Note]

`<ENDPOINT_URL>` is the URL without the bucket name or path. For example: `endpoint=sfo2.digitaloceanspaces.com`.

:::

- **dataset** - The category of logs you want to receive. Refer to [Datasets](/logs/logpush/logpush-job/datasets/) for the full list of supported datasets.
- **output_options** (optional) - To configure fields, sample rate, and timestamp format, refer to [Log Output Options](/logs/logpush/logpush-job/log-output-options/).

Example request using cURL:

<APIRequest
	path="/zones/{zone_id}/logpush/jobs"
	method="POST"
	json={{
		name: "<DOMAIN_NAME>",
		destination_conf:
			"s3://<BUCKET_NAME>/<BUCKET_PATH>?region=<REGION>&access-key-id=<ACCESS_KEY_ID>&secret-access-key=<SECRET_ACCESS_KEY>&endpoint=<ENDPOINT_URL>",
		output_options: {
			field_names: [
				"ClientIP",
				"ClientIP",
				"ClientRequestHost",
				"ClientRequestMethod",
				"ClientRequestURI",
				"EdgeEndTimestamp",
				"EdgeResponseBytes",
				"EdgeResponseStatus",
				"EdgeStartTimestamp",
				"RayID",
			],
			timestamp_format: "rfc3339",
		},
		dataset: "http_requests",
	}}
/>

Response:

```json
{
  "errors": [],
  "messages": [],
  "result": {
    "id": <JOB_ID>,
    "dataset": "http_requests",
    "enabled": false,
    "name": "<DOMAIN_NAME>",
    "output_options": {
      "field_names": ["ClientIP", "ClientRequestHost", "ClientRequestMethod", "ClientRequestURI", "EdgeEndTimestamp","EdgeResponseBytes", "EdgeResponseStatus", "EdgeStartTimestamp", "RayID"],
      "timestamp_format": "rfc3339"
    },
    "destination_conf": "s3://<BUCKET_NAME>/<BUCKET_PATH>?region=<REGION>&access-key-id=<ACCESS_KEY_ID>&secret-access-key=<SECRET_ACCESS_KEY>&endpoint=<ENDPOINT_URL>",
    "last_complete": null,
    "last_error": null,
    "error_message": null
  },
  "success": true
}
```

### 2. Enable (update) a job

To enable a job, make a `PUT` request to the Logpush jobs endpoint. You will use the job ID returned from the previous step in the URL, and send `{"enabled": true}` in the request body.

Example request using cURL:

```bash
curl --request PUT \
https://api.cloudflare.com/client/v4/zones/{zone_id}/logpush/jobs/{job_id} \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "enabled": true
}'
```

Response:

```json
{
  "errors": [],
  "messages": [],
  "result": {
    "id": <JOB_ID>,
    "dataset": "http_requests",
    "enabled": true,
    "name": "<DOMAIN_NAME>",
    "output_options": {
      "field_names": ["ClientIP", "ClientRequestHost", "ClientRequestMethod", "ClientRequestURI", "EdgeEndTimestamp","EdgeResponseBytes", "EdgeResponseStatus", "EdgeStartTimestamp", "RayID"],
      "timestamp_format": "rfc3339"
    },
    "destination_conf": "s3://<BUCKET_NAME>/<BUCKET_PATH>?region=<REGION>&access-key-id=<ACCESS_KEY_ID>&secret-access-key=<SECRET_ACCESS_KEY>&endpoint=<ENDPOINT_URL>",
    "last_complete": null,
    "last_error": null,
    "error_message": null
  },
  "success": true
}
```
